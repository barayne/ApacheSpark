{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark DataFrames Project\n",
    "Instructions\n",
    "As a Data professional, you need to perform an analysis by answering questions about some stock market data on Safaricom from the years 2012-2017. You will need to perform the following:\n",
    "\n",
    "Data Importation and Exploration\n",
    "● Start a spark session and load the stock file while inferring the data types.\n",
    "\n",
    "● Determine the column names\n",
    "\n",
    "● Make observations about the schema.\n",
    "\n",
    "● Show the first 5 rows\n",
    "\n",
    "● Use the describe method to learn about the data frame\n",
    "\n",
    "Data Preparation\n",
    "● Format all the data to 2 decimal places i.e. format_number()\n",
    "\n",
    "● Create a new data frame with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day\n",
    "\n",
    "Data Analysis\n",
    "● What day had the Peak High in Price?\n",
    "\n",
    "● What is the mean of the Close column?\n",
    "\n",
    "● What is the max and min of the Volume column?\n",
    "\n",
    "● How many days was the Close lower than 60 dollars?\n",
    "\n",
    "● What percentage of the time was the High greater than 80 dollars?\n",
    "\n",
    "● What is the Pearson correlation between High and Volume?\n",
    "\n",
    "● What is the max High per year?\n",
    "\n",
    "● What is the average Close for each Calendar Month?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m845.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:09\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/py4j/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m726.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764010 sha256=58008a16e89a6e4ac3c4096230e02574302811a26ef17a46b0e047252d886b8d\n",
      "  Stored in directory: /Users/Barayne/Library/Caches/pip/wheels/05/75/73/81f84d174299abca38dd6a06a5b98b08ae25fce50ab8986fa1\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Pre-requisites\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/31 09:12:09 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.249.121 instead (on interface en0)\n",
      "22/08/31 09:12:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/31 09:12:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Next, we'll run a local spark session\n",
    "# ---\n",
    "#\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Importation and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Pass in the SparkContext object `sc`\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# Read csv data into a DataFrame object `df`\n",
    "df = sqlCtx.read.csv(\"saf_stock.csv\")\n",
    "\n",
    "# Print the type\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"
     ]
    }
   ],
   "source": [
    "# Determine the column names\n",
    "# ---\n",
    "#\n",
    "df = sqlCtx.read.csv(\"saf_stock.csv\", header=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|      Date|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|      1258|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean|      null| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|      null|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|2012-01-03|56.389998999999996|        57.060001|        56.299999|        56.419998|         10010500|        50.363689|\n",
      "|    max|2016-12-30|         90.800003|        90.970001|            89.25|        90.470001|          9994400|84.91421600000001|\n",
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the describe method to learn about the data frame Data Preparation\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+--------+---------+\n",
      "|      Date| Open| High|  Low|Close|  Volume|Adj Close|\n",
      "+----------+-----+-----+-----+-----+--------+---------+\n",
      "|2012-01-03|59.97|61.06|59.87|60.33|12668800|    52.62|\n",
      "|2012-01-04|60.21|60.35|59.47|59.71| 9593300|    52.08|\n",
      "|2012-01-05|59.35|59.62|58.37|59.42|12768200|    51.83|\n",
      "|2012-01-06|59.42|59.45|58.87|59.00| 8069400|    51.46|\n",
      "|2012-01-09|59.03|59.55|58.92|59.18| 6679300|    51.62|\n",
      "+----------+-----+-----+-----+-----+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Format all the data to 2 decimal places i.e. format_number()\n",
    "\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "df_2_decimal=df.select(df['Date'],format_number(df['Open'].cast('float'),2).alias('Open'),\n",
    "              format_number(df['High'].cast('float'), 2).alias('High'),\n",
    "              format_number(df['Low'].cast('float'), 2).alias('Low'),\n",
    "              format_number(df['Close'].cast('float'), 2).alias('Close'),\n",
    "              df['Volume'].cast('int').alias('Volume'),\n",
    "              format_number(df['Adj Close'].cast('float'), 2).alias('Adj Close')\n",
    "              )\n",
    "\n",
    "df_2_decimal.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+--------+---------+--------------------+\n",
      "|      Date| Open| High|  Low|Close|  Volume|Adj Close|            HV Ratio|\n",
      "+----------+-----+-----+-----+-----+--------+---------+--------------------+\n",
      "|2012-01-03|59.97|61.06|59.87|60.33|12668800|    52.62|4.819714574387472E-6|\n",
      "|2012-01-04|60.21|60.35|59.47|59.71| 9593300|    52.08|6.290848821573389...|\n",
      "|2012-01-05|59.35|59.62|58.37|59.42|12768200|    51.83|4.669413073103491E-6|\n",
      "|2012-01-06|59.42|59.45|58.87|59.00| 8069400|    51.46|7.367338339901356E-6|\n",
      "|2012-01-09|59.03|59.55|58.92|59.18| 6679300|    51.62|8.915604928660188E-6|\n",
      "+----------+-----+-----+-----+-----+--------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Create a new data frame with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day\n",
    "\n",
    "new_df = df_2_decimal.withColumn(\"HV Ratio\",df_2_decimal['High']/df_2_decimal['Volume'])\n",
    "print(new_df.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# What day had the Peak High in Price?\n",
    "print(new_df.orderBy(new_df['High'].desc()).head(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844992050863|\n",
      "+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# What is the mean of the Close column?\n",
    "from pyspark.sql.functions import mean, max, min\n",
    "\n",
    "print(new_df.select(mean('Close')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min(Volume)|max(Volume)|\n",
      "+-----------+-----------+\n",
      "|    2094900|   80898100|\n",
      "+-----------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# What is the max and min of the Volume column?\n",
    "print(new_df.select(min('Volume'),max('Volume')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# How many days was the Close lower than 60 dollars?\n",
    "print(new_df.filter(new_df['Close'] < 60).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.426073131955485\n"
     ]
    }
   ],
   "source": [
    "# What percentage of the time was the High greater than 80 dollars?\n",
    "print((new_df.filter(new_df['High']>80).count()/df.count()) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  corr(High, Volume)|\n",
      "+--------------------+\n",
      "|-0.33843260582148915|\n",
      "+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# What is the Pearson correlation between High and Volume?\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "print(new_df.select(corr('High','Volume')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|max(High)|\n",
      "+----+---------+\n",
      "|2012|    77.60|\n",
      "|2013|    81.37|\n",
      "|2014|    88.09|\n",
      "|2015|    90.97|\n",
      "|2016|    75.19|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the max High per year?\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "new_df.groupby(F.date_format('Date','yyyy').alias('Year')).agg({'High': 'max'}).sort('Year').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|       avg(Close)|\n",
      "+-----+-----------------+\n",
      "|   01|71.44801980198022|\n",
      "|   02|71.30680412371134|\n",
      "|   03|71.77794392523363|\n",
      "|   04|72.97361904761907|\n",
      "|   05|72.30971698113206|\n",
      "|   06|72.49537735849057|\n",
      "|   07|74.43971962616824|\n",
      "|   08|73.02981818181819|\n",
      "|   09|72.18411764705883|\n",
      "|   10|71.57854545454546|\n",
      "|   11|72.11108910891085|\n",
      "|   12|72.84792452830189|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What is the average Close for each Calendar Month?\n",
    "\n",
    "new_df.groupby(F.date_format('Date','MM').alias('Month')).agg({'Close': 'mean'}).sort('Month').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
